from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf

# Load a pre-trained BERT model for classification
model_name = "bert-base-uncased"
model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(model_name)


# Freeze all layers except the final classification layer
for layer in model.layers:
    layer.trainable = False

# Unfreeze the final classification layer
model.classifier.trainable = True

# Freeze the first few transformer layers in the BERT model
for layer in model.bert.encoder.layer[:6]:  # Freeze first 6 layers
    for sub_layer in layer.layers:
        sub_layer.trainable = False


for layer in model.layers:
    print(f"Layer: {layer.name}, Trainable: {layer.trainable}")

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Example of encoding text data for the model
texts = ["I love machine learning!", "TensorFlow is great."]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="tf")

# Fine-tune the model
model.fit(inputs['input_ids'], labels, epochs=3, batch_size=16)

# Unfreeze the BERT layers for further fine-tuning
for layer in model.bert.encoder.layer:
    layer.trainable = True

# Re-compile the model after modifying the trainable layers
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

