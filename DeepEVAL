from transformers import AutoTokenizer, AutoModelForSequenceClassification
import tensorflow as tf

# Load a pre-trained evaluation model (e.g., DeepEVAL or similar)
model_name = "gpt-3.5-turbo"  # Replace with actual model name if available
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)  # Regression task if scoring is continuous
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Example text and ratings (regression)
texts = ["The quick brown fox jumps over the lazy dog.", "Hello world!"]
scores = [4, 2]  # Example quality scores (continuous)

# Tokenize input texts
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="tf", max_length=512)

# Convert scores to TensorFlow-compatible format
labels = tf.convert_to_tensor(scores)

# Compile the model (for regression, use MSE loss)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
              loss='mean_squared_error',
              metrics=['mae'])  # Mean Absolute Error (optional for regression)

# Fine-tune the model
model.fit(inputs['input_ids'], labels, epochs=3, batch_size=8)

model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.1),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Example test data
test_texts = ["This is a great story.", "The cat sat on the mat."]
test_scores = [4, 2]

# Tokenize test texts
test_inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors="tf", max_length=512)

# Evaluate the model
loss, mae = model.evaluate(test_inputs['input_ids'], tf.convert_to_tensor(test_scores))
print(f"Test loss: {loss}")
print(f"Test MAE: {mae}")

# Inference (predict quality score for new text)
new_texts = ["The moon shines brightly on a clear night."]
new_inputs = tokenizer(new_texts, padding=True, truncation=True, return_tensors="tf", max_length=512)

# Get predictions
predictions = model.predict(new_inputs['input_ids'])
print(predictions)

