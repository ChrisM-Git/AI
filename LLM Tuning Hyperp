from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf

# Load pre-trained model and tokenizer
model_name = "bert-base-uncased"
model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Hyperparameters
learning_rate = 5e-5
batch_size = 16
epochs = 3
warmup_steps = 500  # Warm-up steps (optional)
weight_decay = 0.01
max_seq_length = 128  # Maximum sequence length for input texts


# Example text data
texts = ["I love machine learning!", "TensorFlow is great."]
labels = [1, 0]  # Binary labels for classification

# Tokenize input texts
inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_seq_length, return_tensors="tf")

# Define AdamW optimizer with weight decay
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

# Optionally, use learning rate scheduler (e.g., Cosine Decay or Linear Warm-up)
lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=learning_rate,
    decay_steps=1000,
    decay_rate=0.95,
    staircase=True
)

# Compile the model
model.compile(optimizer=optimizer,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# Early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

# Learning rate reduction
lr_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-7)


# Fine-tune the model
model.fit(inputs['input_ids'], labels, epochs=epochs, batch_size=batch_size, 
          validation_split=0.1,  # Optionally use a validation set
          callbacks=[early_stopping, lr_reduction])

# Evaluate the model on a test set (example)
test_texts = ["This is amazing!", "Not so good."]
test_inputs = tokenizer(test_texts, padding=True, truncation=True, max_length=max_seq_length, return_tensors="tf")
test_labels = [1, 0]  # Example labels

loss, accuracy = model.evaluate(test_inputs['input_ids'], test_labels)
print(f"Test loss: {loss}")
print(f"Test accuracy: {accuracy}")

from kerastuner import HyperModel, RandomSearch

class MyHyperModel(HyperModel):
    def build(self, hp):
        model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')),
                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      metrics=['accuracy'])
        return model

tuner = RandomSearch(MyHyperModel(), objective='val_accuracy', max_trials=10)
tuner.search(inputs['input_ids'], labels, epochs=3, batch_size=16, validation_split=0.1)
best_model = tuner.get_best_models(1)[0]

