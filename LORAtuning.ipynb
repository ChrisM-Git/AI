{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisM-Git/AI/blob/main/LORAtuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trelis Livestream\n",
        "\n",
        "Most weeks at 5 pm Irish time on [YouTube](youtube.com/@TrelisResearch) and [X](twitter.com/@TrelisResearch).\n",
        "\n",
        "Find other fine-tuning/inference resources at [Trelis.com](Trelis.com/About).\n",
        "\n",
        "[RunPod Affiliate Link](https://runpod.io?ref=jmfkcdio\n",
        ") (supports the channel)."
      ],
      "metadata": {
        "id": "VVrfIVqLkq84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# LoRA Fine-tuning Parameter Selection\n",
        "Full fine-tuning.\n",
        "- Choosing learning rate.\n",
        "\n",
        "LoRA Settings:\n",
        "- [visual examples.](https://docs.google.com/presentation/d/18npsbcrkCi3kY41ejxquEqR6TjLPFk9l6nwSavaXN1w/edit?usp=sharing)\n",
        "- What are LoRA modules?\n",
        "- LoRA alpha and LoRA r (rank).\n",
        "- rank stabilized LoRA.\n",
        "- choosing alpha.\n",
        "- choosing r based on the application\n",
        "- making lm_head and embed_tokens trainable.\n",
        "\n",
        "Batch size:\n",
        "- Pros and cons of increasing/decreasing.\n",
        "- Gradient Accumulation.\n",
        "\n",
        "Dataset sizes and epochs:\n",
        "- How dataset size required depends on training type.\n",
        "- Number of epochs.\n",
        "\n",
        "Learning rate scheduler:\n",
        "- constant vs cosine vs annealing.\n",
        "\n",
        "Training examples:\n",
        "- [wandb](https://wandb.ai/trelis/function-calling-v3b?nw=nwuserronankmcgovern)\n",
        "\n",
        "\n",
        "### Key Links:\n",
        "- [ADVANCED Fine-tuning Repo](https://trelis.com/ADVANCED-fine-tuning/)\n",
        "- [One click RunPod templates](github.com/TrelisResearch/one-click-llms) - quick note on last week and how to inference with two images."
      ],
      "metadata": {
        "id": "T10msX0Ck5-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate transformers hf_transfer -q\n",
        "# !pip install flash-attn -q"
      ],
      "metadata": {
        "id": "9Cc9YNEDCX5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17242f7c-cace-43b0-d920-5ebb63398b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Setting the environment variable to enable fast downloads with the hf_transfer rust library.\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ],
      "metadata": {
        "id": "SH6i0vTLAs-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Xlbc9ekmK2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586a4ea6-c16d-42b6-fb97-9888af008545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    cache_dir='',\n",
        "    device_map='auto',\n",
        "    # attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir='')"
      ],
      "metadata": {
        "id": "lx7vD3bGBVhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7n8Jyczzji4",
        "outputId": "67d8e207-0d9a-45fe-e9f7-641511fe41dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-21): 22 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
            "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqXN_FW-0CtS",
        "outputId": "4e424a49-f0cb-4552-992c-68e40634455d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/251.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/251.6 kB\u001b[0m \u001b[31m821.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/251.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m245.8/251.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    target_modules=['gate_proj','k_proj'],\n",
        "    lora_dropout=0.1, #prevent overfitting. Not supported unsloth.\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=['lm_head','embed_tokens'],\n",
        "    r=128, # rank\n",
        "    lora_alpha=32,\n",
        "    use_rslora=True,\n",
        ")"
      ],
      "metadata": {
        "id": "WA86cahAzkm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Learning rates\n",
        "\n",
        "# base_learning_rate=1e-4\n",
        "\n",
        "# # base rule of thumb\n",
        "# adapter_learning_rate= base_learning_rate * alpha / r = base_rate * 2.\n",
        "\n",
        "# # rank stabilised lora\n",
        "# adapter_learning_rate = alpha * base_learning_rate / sqrt(r)\n",
        "\n",
        "# # setting alpha\n",
        "# alpha ~ sqrt(r - where r is the rough size of the original matrix)"
      ],
      "metadata": {
        "id": "8ZOztnbYVBNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 2000**0.5\n",
        "print(alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaLQ9BfnWTHw",
        "outputId": "026854b6-5b46-4c4d-97a3-2a2fe9635991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44.721359549995796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model\n",
        "model=get_peft_model(model,lora_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEiJs6mH1A2G",
        "outputId": "bd25408d-22e9-4a0d-d6eb-c12d58802c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 159,186,944 || all params: 1,259,235,328 || trainable%: 12.6416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/trl.git -q -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn0mc8P41X6V",
        "outputId": "7fbd18c8-b723-45f8-9430-481e7dc7b1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "QiODzn9WXcAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"Trelis/openassistant-llama-style\"\n",
        "\n",
        "train_dataset = load_dataset(dataset_name, split=\"train[:100]\")\n",
        "eval_dataset = load_dataset(dataset_name, split=\"test[:10]\")\n",
        "\n",
        "print(train_dataset)\n",
        "print(eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pXwn0dp1cWc",
        "outputId": "6aec07d6-ceb1-4b0c-dd46-dcd321ee63c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 100\n",
            "})\n",
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 10\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# docs are here: https://huggingface.co/docs/trl/en/sft_trainer\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    args=SFTConfig(\n",
        "        # max_steps=1,\n",
        "        max_seq_length=2048,\n",
        "        dataset_text_field=\"text\",\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        do_eval=True,\n",
        "        lr_scheduler_type=\"constant\",\n",
        "        learning_rate=1e-4,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=0.2,\n",
        "        warmup_ratio=0.05,\n",
        "        output_dir='./',\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": True}, # trl will soon require this (true is faster but more complicated and bug prone)\n",
        "        logging_steps=1,\n",
        "    )\n",
        "    # peft_config=lora_config, #no need to pass this in if you have already applied the peft config.\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "if5853dx1xXv",
        "outputId": "a0bb3c97-a04d-45a6-cc30-930ef67153e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 03:40, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.823500</td>\n",
              "      <td>1.518413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.635000</td>\n",
              "      <td>1.484386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.617400</td>\n",
              "      <td>1.461405</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=1.6919945081075032, metrics={'train_runtime': 337.3955, 'train_samples_per_second': 0.296, 'train_steps_per_second': 0.009, 'total_flos': 598431981010944.0, 'train_loss': 1.6919945081075032, 'epoch': 0.9230769230769231})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "khBAQdTS3y1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}