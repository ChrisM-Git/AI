from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core import get_response_synthesizer
from llama_index.core.response_synthesizers import ResponseMode
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
from dotenv import load_dotenv
import os

# Load the OpenAI API Key into the environment variable named OPENAI_API_KEY
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")



# Set the OpenAI model and temperature
# The Settings is a bundle of commonly used resources used during the indexing and querying stage in a
# LlamaIndex pipeline/application. You can use it to set the global configuration.
Settings.llm = OpenAI(temperature=0.7, model="llama2")

# load data using SimpleDirectoryReader
# All files in the data folder are loaded into the index
# each page is a document in the index
documents = SimpleDirectoryReader("docs").load_data()
index = VectorStoreIndex.from_documents(
    documents,
)

# configure retriever
# The retriever is used to retrieve the most similar documents to a query
# The similarity_top_k parameter specifies the number of most similar documents to retrieve
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=10,
)
# The response synthesizer is used to turn the response data into a human-readable format
response_synthesizer = get_response_synthesizer(response_mode=ResponseMode.COMPACT)


query_engine = RetrieverQueryEngine(
    retriever=retriever,
    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7, # filter nodes with similarity score below the cutoff
                                                 filter_empty=True,  # filter empty nodes
                                                 filter_duplicates=True,  # filter duplicate nodes
                                                 filter_similar=True,  # filter similar nodes
                                                 )],
    response_synthesizer=response_synthesizer,
)

# The query engine is used to query the index and generate a response

response = query_engine.query("What are the Terms of payments?")
print(response)
from fastapi import FastAPI
from langchain_community.chat_models import ChatOllama

from langserve import add_routes

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="Spin up a simple api server using Langchain's Runnable interfaces",
)


llm = ChatOllama(model="llama2")

add_routes(
    app,
    llm,
    path="/ollama",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
